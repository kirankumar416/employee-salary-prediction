{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cea855f-669a-40d4-b168-810c7e202610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Data Loading Complete: adult 3.csv loaded.\n",
      "First 5 rows of the dataset:\n",
      "   age  workclass  fnlwgt     education  educational-num      marital-status  \\\n",
      "0   25    Private  226802          11th                7       Never-married   \n",
      "1   38    Private   89814       HS-grad                9  Married-civ-spouse   \n",
      "2   28  Local-gov  336951    Assoc-acdm               12  Married-civ-spouse   \n",
      "3   44    Private  160323  Some-college               10  Married-civ-spouse   \n",
      "4   18          ?  103497  Some-college               10       Never-married   \n",
      "\n",
      "          occupation relationship   race  gender  capital-gain  capital-loss  \\\n",
      "0  Machine-op-inspct    Own-child  Black    Male             0             0   \n",
      "1    Farming-fishing      Husband  White    Male             0             0   \n",
      "2    Protective-serv      Husband  White    Male             0             0   \n",
      "3  Machine-op-inspct      Husband  Black    Male          7688             0   \n",
      "4                  ?    Own-child  White  Female             0             0   \n",
      "\n",
      "   hours-per-week native-country income  \n",
      "0              40  United-States  <=50K  \n",
      "1              50  United-States  <=50K  \n",
      "2              40  United-States   >50K  \n",
      "3              40  United-States   >50K  \n",
      "4              30  United-States  <=50K  \n",
      "\n",
      "Dataset Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 48842 entries, 0 to 48841\n",
      "Data columns (total 15 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   age              48842 non-null  int64 \n",
      " 1   workclass        48842 non-null  object\n",
      " 2   fnlwgt           48842 non-null  int64 \n",
      " 3   education        48842 non-null  object\n",
      " 4   educational-num  48842 non-null  int64 \n",
      " 5   marital-status   48842 non-null  object\n",
      " 6   occupation       48842 non-null  object\n",
      " 7   relationship     48842 non-null  object\n",
      " 8   race             48842 non-null  object\n",
      " 9   gender           48842 non-null  object\n",
      " 10  capital-gain     48842 non-null  int64 \n",
      " 11  capital-loss     48842 non-null  int64 \n",
      " 12  hours-per-week   48842 non-null  int64 \n",
      " 13  native-country   48842 non-null  object\n",
      " 14  income           48842 non-null  object\n",
      "dtypes: int64(6), object(9)\n",
      "memory usage: 5.6+ MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\karth\\AppData\\Local\\Temp\\ipykernel_24332\\3923644171.py:44: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(df[col].mode()[0], inplace=True) # Mode for categorical\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Data Cleaning and Missing Value Imputation Complete.\n",
      "\n",
      "Dataset Info after cleaning:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 48842 entries, 0 to 48841\n",
      "Data columns (total 15 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   age              48842 non-null  int64 \n",
      " 1   workclass        48842 non-null  object\n",
      " 2   fnlwgt           48842 non-null  int64 \n",
      " 3   education        48842 non-null  object\n",
      " 4   educational-num  48842 non-null  int64 \n",
      " 5   marital-status   48842 non-null  object\n",
      " 6   occupation       48842 non-null  object\n",
      " 7   relationship     48842 non-null  object\n",
      " 8   race             48842 non-null  object\n",
      " 9   gender           48842 non-null  object\n",
      " 10  capital-gain     48842 non-null  int64 \n",
      " 11  capital-loss     48842 non-null  int64 \n",
      " 12  hours-per-week   48842 non-null  int64 \n",
      " 13  native-country   48842 non-null  object\n",
      " 14  income           48842 non-null  object\n",
      "dtypes: int64(6), object(9)\n",
      "memory usage: 5.6+ MB\n",
      "\n",
      "Income classes: ['<=50K' '>50K']\n",
      "\n",
      "✅ Data split into Training (39073 samples) and Testing (9769 samples).\n",
      "Training target distribution:\n",
      "0    0.76073\n",
      "1    0.23927\n",
      "Name: proportion, dtype: float64\n",
      "Testing target distribution:\n",
      "0    0.760672\n",
      "1    0.239328\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "✅ Preprocessing steps defined with ColumnTransformer for adult dataset.\n",
      "\n",
      "🚀 Training and Evaluating Logistic Regression...\n",
      "  Performing 5-fold Cross-Validation...\n",
      "  CV Accuracy (Mean ± Std): 0.8510 ± 0.0037\n",
      "  ✅ Logistic Regression Test Set Evaluation Results:\n",
      "    Accuracy: 0.8523\n",
      "    Precision: 0.7407\n",
      "    Recall: 0.5890\n",
      "    F1-Score: 0.6562\n",
      "    ROC AUC: 0.9042\n",
      "    Confusion Matrix:\n",
      "[[6949  482]\n",
      " [ 961 1377]]\n",
      "\n",
      "🚀 Training and Evaluating Random Forest Classifier...\n",
      "  Performing 5-fold Cross-Validation...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "\n",
    "# Suppress specific warnings for cleaner output (optional, but good for final presentation)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='sklearn')\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module='pandas')\n",
    "\n",
    "# --- 1. Data Loading ---\n",
    "try:\n",
    "    df = pd.read_csv(r\"C:\\Users\\karth\\Downloads\\adult 3.csv\")\n",
    "    print(\"✅ Data Loading Complete: adult 3.csv loaded.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'adult 3.csv' not found. Please ensure the file is in the correct directory.\")\n",
    "    exit()\n",
    "\n",
    "print(\"First 5 rows of the dataset:\")\n",
    "print(df.head())\n",
    "print(\"\\nDataset Info:\")\n",
    "df.info()\n",
    "\n",
    "# --- 2. Data Cleaning and Initial Preprocessing ---\n",
    "df.columns = df.columns.str.strip() # Remove leading/trailing spaces from column names\n",
    "\n",
    "for col in df.select_dtypes(include='object').columns:\n",
    "    df[col] = df[col].str.strip() # Remove leading/trailing spaces from string values\n",
    "\n",
    "df.replace('?', np.nan, inplace=True)\n",
    "\n",
    "# Impute missing values\n",
    "for col in df.select_dtypes(include='object').columns:\n",
    "    if df[col].isnull().sum() > 0:\n",
    "        df[col].fillna(df[col].mode()[0], inplace=True) # Mode for categorical\n",
    "\n",
    "for col in df.select_dtypes(include=['int64', 'float64']).columns:\n",
    "    if df[col].isnull().sum() > 0:\n",
    "        df[col].fillna(df[col].median(), inplace=True) # Median for numerical\n",
    "\n",
    "print(\"\\n✅ Data Cleaning and Missing Value Imputation Complete.\")\n",
    "print(\"\\nDataset Info after cleaning:\")\n",
    "df.info()\n",
    "\n",
    "# --- 3. Separate Features (X) and Target (y) & Split Data ---\n",
    "X = df.drop('income', axis=1)\n",
    "y = df['income']\n",
    "\n",
    "# Address FutureWarning: Ensure y is a 1D array/Series\n",
    "# .values.ravel() converts a Series/DataFrame column to a 1D numpy array\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y.values.ravel()) # Added .values.ravel() here\n",
    "print(f\"\\nIncome classes: {le.classes_}\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded)\n",
    "\n",
    "# Addressing UserWarning: X does not have valid feature names...\n",
    "# It's good practice to keep X_train and X_test as DataFrames with column names.\n",
    "# The pipeline should handle the names correctly if X is a DataFrame.\n",
    "# If the warning persists, it means the ColumnTransformer is dropping them\n",
    "# or the model expects numpy arrays directly without names.\n",
    "# For now, ensure X_train and X_test are passed as DataFrames.\n",
    "\n",
    "print(f\"\\n✅ Data split into Training ({len(X_train)} samples) and Testing ({len(X_test)} samples).\")\n",
    "print(f\"Training target distribution:\\n{pd.Series(y_train).value_counts(normalize=True)}\")\n",
    "print(f\"Testing target distribution:\\n{pd.Series(y_test).value_counts(normalize=True)}\")\n",
    "\n",
    "\n",
    "# --- 4. Define Preprocessing Steps using ColumnTransformer ---\n",
    "numerical_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist() # Ensure lists for ColumnTransformer\n",
    "categorical_features = X.select_dtypes(include=['object']).columns.tolist() # Ensure lists for ColumnTransformer\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
    "    ],\n",
    "    remainder='passthrough',\n",
    "    # Ensure ColumnTransformer outputs a DataFrame with feature names (for debugging)\n",
    "    # This might require scikit-learn >= 0.23 for set_output(transform=\"pandas\")\n",
    "    # For now, we rely on the pipeline to handle it.\n",
    ")\n",
    "\n",
    "print(\"\\n✅ Preprocessing steps defined with ColumnTransformer for adult dataset.\")\n",
    "\n",
    "# --- 5. Create and Evaluate Classification Models with Pipelines and Cross-Validation ---\n",
    "\n",
    "model_results = {}\n",
    "all_metrics_df = pd.DataFrame() # To store all detailed metrics for graphing\n",
    "\n",
    "def train_evaluate_classification_pipeline(name, model_pipeline, X_train, y_train, X_test, y_test, cv_folds=5):\n",
    "    print(f\"\\n🚀 Training and Evaluating {name}...\")\n",
    "\n",
    "    print(f\"  Performing {cv_folds}-fold Cross-Validation...\")\n",
    "    cv_scores = cross_val_score(model_pipeline, X_train, y_train, cv=cv_folds, scoring='accuracy', n_jobs=-1)\n",
    "    print(f\"  CV Accuracy (Mean ± Std): {cv_scores.mean():.4f} ± {cv_scores.std():.4f}\")\n",
    "\n",
    "    model_pipeline.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = model_pipeline.predict(X_test)\n",
    "    y_prob = model_pipeline.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    roc_auc = roc_auc_score(y_test, y_prob)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    print(f\"  ✅ {name} Test Set Evaluation Results:\")\n",
    "    print(f\"    Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"    Precision: {precision:.4f}\")\n",
    "    print(f\"    Recall: {recall:.4f}\")\n",
    "    print(f\"    F1-Score: {f1:.4f}\")\n",
    "    print(f\"    ROC AUC: {roc_auc:.4f}\")\n",
    "    print(f\"    Confusion Matrix:\\n{cm}\")\n",
    "\n",
    "    # Store results for model comparison graph\n",
    "    model_results[name] = {\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1-Score': f1,\n",
    "        'ROC AUC': roc_auc,\n",
    "        'CV_Accuracy_Mean': cv_scores.mean(),\n",
    "        'CV_Accuracy_Std': cv_scores.std()\n",
    "    }\n",
    "    return model_pipeline, y_pred\n",
    "\n",
    "models = {\n",
    "    \"Logistic Regression\": Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                                           ('classifier', LogisticRegression(random_state=42, n_jobs=-1, solver='liblinear'))]),\n",
    "\n",
    "    \"Random Forest Classifier\": Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                                                ('classifier', RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1))]),\n",
    "\n",
    "    \"XGBoost Classifier\": Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                                          ('classifier', XGBClassifier(objective='binary:logistic', eval_metric='logloss',\n",
    "                                                                        n_estimators=100, learning_rate=0.1, random_state=42, n_jobs=-1))])\n",
    "}\n",
    "\n",
    "trained_models = {}\n",
    "predictions = {}\n",
    "\n",
    "for name, pipeline in models.items():\n",
    "    trained_pipeline, y_pred = train_evaluate_classification_pipeline(name, pipeline, X_train, y_train, X_test, y_test)\n",
    "    trained_models[name] = trained_pipeline\n",
    "    predictions[name] = y_pred\n",
    "\n",
    "print(\"\\n--- 📊 Summary of Model Performance ---\")\n",
    "results_df = pd.DataFrame(model_results).T\n",
    "results_df = results_df.sort_values(by='Accuracy', ascending=False)\n",
    "print(results_df.round(4))\n",
    "\n",
    "# --- Model Comparison Graphs ---\n",
    "print(\"\\n--- 📈 Generating Model Comparison Graphs ---\")\n",
    "\n",
    "# Plot 1: Bar chart for Accuracy, F1-Score, and ROC AUC\n",
    "metrics_to_plot = ['Accuracy', 'F1-Score', 'ROC AUC']\n",
    "plot_df = results_df[metrics_to_plot]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "plot_df.plot(kind='bar', ax=ax, rot=0)\n",
    "ax.set_title('Model Performance Comparison (Test Set)')\n",
    "ax.set_ylabel('Score')\n",
    "ax.legend(title='Metrics')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot 2: Bar chart for Cross-Validation Accuracy\n",
    "cv_plot_df = results_df[['CV_Accuracy_Mean']]\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "cv_plot_df.plot(kind='bar', ax=ax, rot=0, yerr=results_df['CV_Accuracy_Std'], capsize=4) # Include Std Dev as error bars\n",
    "ax.set_title('Cross-Validation Accuracy Comparison')\n",
    "ax.set_ylabel('Accuracy Score')\n",
    "ax.legend(title='Metric')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "best_model_name = results_df.index[0]\n",
    "best_accuracy_score = results_df.loc[best_model_name, 'Accuracy']\n",
    "\n",
    "print(f\"\\n👑 Based on Accuracy Score, the best performing model is: *{best_model_name}* with Accuracy = {best_accuracy_score:.4f}\")\n",
    "\n",
    "# --- 6. Optional: Hyperparameter Tuning for the Best Model ---\n",
    "print(f\"\\n--- ⚙ Hyperparameter Tuning for {best_model_name} ---\")\n",
    "\n",
    "param_grid = {}\n",
    "\n",
    "if best_model_name == \"XGBoost Classifier\":\n",
    "    param_grid = {\n",
    "        'classifier__n_estimators': [100, 200],\n",
    "        'classifier__learning_rate': [0.05, 0.1],\n",
    "        'classifier__max_depth': [3, 5]\n",
    "    }\n",
    "    tuned_model_pipeline = trained_models[best_model_name]\n",
    "elif best_model_name == \"Random Forest Classifier\":\n",
    "    param_grid = {\n",
    "        'classifier__n_estimators': [100, 200],\n",
    "        'classifier__max_features': [0.7, 1.0],\n",
    "        'classifier__max_depth': [10, None]\n",
    "    }\n",
    "    tuned_model_pipeline = trained_models[best_model_name]\n",
    "else: # Logistic Regression\n",
    "    print(\"Logistic Regression typically doesn't require extensive hyperparameter tuning like tree-based models for basic usage.\")\n",
    "    param_grid = {} # No tuning for this example\n",
    "    best_tuned_model = trained_models[best_model_name] # Assign directly if no tuning\n",
    "\n",
    "if param_grid:\n",
    "    print(f\"  Searching for best parameters for {best_model_name}...\")\n",
    "    grid_search = GridSearchCV(tuned_model_pipeline, param_grid, cv=3, scoring='accuracy', n_jobs=-1, verbose=1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    print(\"\\nBest hyperparameters found:\", grid_search.best_params_)\n",
    "    print(f\"Best cross-validation Accuracy: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "    best_tuned_model = grid_search.best_estimator_\n",
    "    y_pred_tuned = best_tuned_model.predict(X_test)\n",
    "    y_prob_tuned = best_tuned_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    accuracy_tuned = accuracy_score(y_test, y_pred_tuned)\n",
    "    roc_auc_tuned = roc_auc_score(y_test, y_prob_tuned)\n",
    "\n",
    "    print(f\"\\n✅ Tuned {best_model_name} Test Set Evaluation Results:\")\n",
    "    print(f\"    Accuracy: {accuracy_tuned:.4f}\")\n",
    "    print(f\"    ROC AUC: {roc_auc_tuned:.4f}\")\n",
    "else:\n",
    "    print(f\"Skipping hyperparameter tuning for {best_model_name} as no param_grid was defined.\")\n",
    "\n",
    "\n",
    "# --- 7. Feature Importance (for Tree-based classifiers) ---\n",
    "print(\"\\n--- 🔍 Feature Importance ---\")\n",
    "if best_model_name in [\"Random Forest Classifier\", \"XGBoost Classifier\"]:\n",
    "    preprocessor_transformer = best_tuned_model.named_steps['preprocessor']\n",
    "    \n",
    "    # Get feature names from OneHotEncoder if it exists\n",
    "    encoded_feature_names = []\n",
    "    for name, transformer, features in preprocessor_transformer.transformers_:\n",
    "        if name == 'cat' and hasattr(transformer, 'get_feature_names_out'):\n",
    "            try:\n",
    "                encoded_feature_names = transformer.get_feature_names_out(features)\n",
    "            except AttributeError:\n",
    "                # Fallback for older scikit-learn versions if get_feature_names_out isn't available\n",
    "                print(\"Warning: get_feature_names_out() not found for OneHotEncoder. Feature names might be generic.\")\n",
    "                pass\n",
    "\n",
    "    final_feature_names = numerical_features + encoded_feature_names.tolist() # Combine numerical and encoded categorical names\n",
    "\n",
    "    importances = best_tuned_model.named_steps['classifier'].feature_importances_\n",
    "    \n",
    "    # Ensure importances match feature names length\n",
    "    if len(importances) == len(final_feature_names):\n",
    "        feature_importances_df = pd.DataFrame({'Feature': final_feature_names, 'Importance': importances})\n",
    "        feature_importances_df = feature_importances_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "        print(\"Top 10 Feature Importances:\")\n",
    "        print(feature_importances_df.head(10))\n",
    "\n",
    "        plt.figure(figsize=(12, 7))\n",
    "        sns.barplot(x='Importance', y='Feature', data=feature_importances_df.head(10))\n",
    "        plt.title(f'Top 10 Feature Importances for {best_model_name}')\n",
    "        plt.xlabel('Importance')\n",
    "        plt.ylabel('Feature')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"Could not generate feature importance plot: Mismatch between number of features and importances.\")\n",
    "\n",
    "elif best_model_name == \"Logistic Regression\":\n",
    "    print(\"For Logistic Regression, coefficients indicate feature importance (after scaling).\")\n",
    "    # To get interpretable coefficients for Logistic Regression:\n",
    "    # You would need to apply the preprocessor to a dummy X and then get feature names.\n",
    "    # coefs = best_tuned_model.named_steps['classifier'].coef_[0]\n",
    "    # transformed_features = best_tuned_model.named_steps['preprocessor'].transform(X.head(1))\n",
    "    # feature_names_out = best_tuned_model.named_steps['preprocessor'].get_feature_names_out()\n",
    "    # coef_df = pd.DataFrame({'Feature': feature_names_out, 'Coefficient': coefs}).sort_values(by='Coefficient', ascending=False)\n",
    "    # print(coef_df.head(10))\n",
    "else:\n",
    "    print(f\"Feature importance is not directly applicable in the same way for {best_model_name}.\")\n",
    "\n",
    "\n",
    "# --- 8. Save the Best Model ---\n",
    "model_filename = f'{best_model_name.replace(\" \", \"_\").lower()}_income_classifier_model.joblib'\n",
    "joblib.dump(best_tuned_model, model_filename)\n",
    "print(f\"\\n✅ Best model saved as '{model_filename}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ead1a0-927c-4bf8-b36b-aadd42581741",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
